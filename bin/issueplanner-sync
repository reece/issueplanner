#!/usr/bin/env python

from __future__ import absolute_import, division, print_function, unicode_literals

__doc__ = """Update a GNOME planner XML document with data from Bitbucket issues

Desired result: A Gantt chart in which tasks may optionally correspond
to issues in multiple Bitbucket issue trackers and task data may be
refreshed periodically.  Dependency information, which is not
available in Bitbucket, is stored in the planner XML.

> Integration/User-Features
  > Feature1
  > Feature2
> Project A1
  > Milestone A1
    > Task A1.1
    > Task A1.2
  > Milestone A2
    ...


Process notes:
- fetch all issues

- 3 cases: new, updated, deleted
  new: add to milestone or "unplanned" section
    (add milestone if needed... see below)
  updated: update title, kind, dates, status, owner
    (add owner if needed)  
  deleted: log msg only

- adding a milestone: add as dependent on inferred previous

"""

from argparse import ArgumentParser
from ConfigParser import ConfigParser
import cPickle as pickle
import gzip
import io
import logging
import os
import re
import sys

import lxml.etree as le

from issueplanner.bitbucketclient import BitbucketClient
from issueplanner.issueplannerdoc import IssuePlannerDoc
from issueplanner.utils import bb_to_planner_ts, issue_elapsed_work_seconds

completed_statuses = ['resolved', 'closed', 'wontfix', 'duplicate']

def get_options(argv):
    cfg = get_config()

    ap = ArgumentParser(
        description = __doc__.splitlines()[0],
        )
    ap.add_argument(
        "--planner-filename", "-p",
        required=True,
        help="path to planner xml file"
        )
    ap.add_argument(
        "--bitbucket-username",
        default=cfg.get("bitbucket","username"),
        )
    ap.add_argument(
        "--bitbucket-password",
        default=cfg.get("bitbucket","password"),
        )
    ap.add_argument(
        "--cache-dir", "-C",
        default=cfg.get("issueplanner", "cache-dir"),
        )
    ap.add_argument(
        "--update-cache", "-U",
        action="store_true",
        default=False
        )
    ap.add_argument(
        "--output-filename", "-o",
        )
    ap.add_argument(
        "--prefixes", "-P",
        default=[],
        action="append"
        )
    opts = ap.parse_args(argv[1:])
    opts.cache_dir = os.path.expanduser(opts.cache_dir)
    opts._cfg = cfg
    return opts
    

def get_config():
    cfg_fn = os.path.join(os.path.expanduser("~"), ".config", "issueplanner.cfg")
    cfg = ConfigParser()
    with io.open(cfg_fn,"r") as fp:
        cfg.readfp(fp)
    return cfg


def _get_issues(opts,tracker):
    fqtn = tracker["owner"] + "/" + tracker["slug"]
    pickle_path = os.path.join(opts.cache_dir,fqtn.replace("/","*")+".gz")
    if not opts.update_cache and os.path.exists(pickle_path):
        with gzip.open(pickle_path,"r") as fh:
            issues = pickle.load(fh)
        logger.info("read {n} {fqtn} issues from {fn}".format(
            n=len(issues), fqtn=fqtn, fn=pickle_path))
    else:
        bb = BitbucketClient(username=opts.bitbucket_username,
                             password=opts.bitbucket_password)
        issues = list(bb.get_all_issues(tracker["owner"], tracker["slug"]))
        with gzip.open(pickle_path,"w") as fh:
            pickle.dump(issues,fh)
        logger.info("fetched {n} {fqtn} issues; pickled in {fn}".format(
            n=len(issues), fqtn=fqtn, fn=pickle_path))
    return issues


if __name__ == "__main__":
    logging.basicConfig(level=logging.WARN)
    logger = logging.getLogger(__name__)

    opts = get_options(sys.argv)

    if not os.path.exists(opts.cache_dir):
        os.makedirs(opts.cache_dir)

    with io.open(opts.planner_filename,"r") as fh:
        parser = le.XMLParser(remove_blank_text=True)
        doc = le.parse(fh, parser)
        pd = IssuePlannerDoc(doc.getroot())

    trackers = pd.get_trackers()
    pt_map = {t["prefix"]: t for t in trackers}
    prefixes = opts.prefixes or pt_map.keys()

    all_tasks = pd.tasks()

    for prefix in prefixes:
        logger.info("processing {} issues...".format(prefix))

        pt = pt_map[prefix]
        prj_te = pd.get_project_task(pt)

        # map issue ids => task node elements
        task_issue_re = re.compile(r"(?P<task_title>.*)\s*\[(?P<issue_id>{prefix}-\d+)\]\s*$".format(prefix=prefix))
        planner_tasks_map = {
            m.group("issue_id"): t
            for t,m in ((t, task_issue_re.match(t.get("name"))) for t in all_tasks)
            if m is not None
            }
        planner_tasks_ii = set(planner_tasks_map.keys())

        # map issue ids => issue data
        issues = _get_issues(opts,pt_map[prefix])
        tracker_issues_map = {
            "{}-{}".format(prefix,i["local_id"]): i
            for i in issues
            }
        tracker_issues_ii = set(tracker_issues_map.keys())

        t_not_p = tracker_issues_ii - planner_tasks_ii  # in tracker, not planner
        p_and_t = planner_tasks_ii & tracker_issues_ii  # in both
        p_not_t = planner_tasks_ii - tracker_issues_ii  # in planner, not tracker

        logger.info("{} unique in tracker, {} unique in planner xml, {} common".format(
            len(t_not_p), len(p_not_t), len(p_and_t)))

        if len(p_not_t) > 0:
            logger.warning("{} issue ids in planner are non-existant in the {}/{} issue tracker: {}".format(
                len(p_not_t), pt_map[prefix]["owner"], pt_map[prefix]["slug"], ", ".join(p_not_t)))
            

        # create placeholder tasks for issues that don't exist
        # attributes will be updated in the block below
        for ii in t_not_p:
            issue = tracker_issues_map[ii]
            i_te = pd.create_task(ii)
            prj_te.append(i_te)
            planner_tasks_map[ii] = i_te
            logger.info("added issue {}".format(ii))

        # update these tasks
        for ii in p_and_t ^ t_not_p:
            task = planner_tasks_map[ii]
            issue = tracker_issues_map[ii]

            task.attrib["name"] = "{it} [{ii}]".format(it=issue["title"], ii=ii)

            # set dates and completion
            if not task.attrib["start"]:
                task.attrib["start"] = bb_to_planner_ts(issue["utc_created_on"])
                pd.set_task_constraint(task, "start-no-earlier-than", task.attrib["start"])
            if not task.attrib["work-start"]:
                task.attrib["work-start"] = task.attrib["start"]
            if not task.attrib["end"]:
                task.attrib["end"] = bb_to_planner_ts(issue["utc_last_updated"])
            
            task.attrib["work"] = unicode(issue_elapsed_work_seconds(issue))
            task.attrib["percent-complete"] = "100" if issue["status"] in completed_statuses else "0"

            # move task to correct milestone if neccessary
            ms = issue['metadata']['milestone']
            current_te = task.getparent()
            new_ms_te = pd.get_milestone_task(prj_te, ms)
            new_te = pd.get_task_path(new_ms_te, [issue['metadata']['kind']])
            if current_te != new_te:
                current_te.remove(task)
                new_te.append(task)
                logger.info("moved issue {} from {} to {}".format(
                    ii, current_te.get("name"), new_te.get("name")))

            # assign/unassign task
            

    pd.reset_project_start()

    out_fn = opts.output_filename or opts.planner_filename
    if os.path.exists(out_fn):
        os.rename(out_fn, out_fn + ".bak")
    with io.open(out_fn,"w") as fh:
        fh.write(unicode(pd))
    logger.info("wrote {out_fn}".format(out_fn=out_fn))

