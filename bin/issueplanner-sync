#!/usr/bin/env python

"""Update a GNOME planner XML document with data from Bitbucket issues

Desired result: A Gantt chart in which tasks may optionally correspond
to issues in multiple Bitbucket issue trackers and task data may be
refreshed periodically.  Dependency information, which is not
available in Bitbucket, is stored in the planner XML.

> Integration/User-Features
  > Feature1
  > Feature2
> Project A1
  > Milestone A1
    > Task A1.1
    > Task A1.2
  > Milestone A2
    ...


Process notes:
- fetch all issues

- 3 cases: new, updated, deleted
  new: add to milestone or "unplanned" section
    (add milestone if needed... see below)
  updated: update title, kind, dates, status, owner
    (add owner if needed)  
  deleted: log msg only

- adding a milestone: add as dependent on inferred previous

"""

from argparse import ArgumentParser
from ConfigParser import ConfigParser
import cPickle as pickle
import gzip
import logging
import os
import re
import sys

import lxml.etree as le

from issueplanner.bitbucketclient import BitbucketClient
from issueplanner.issueplannerdoc import IssuePlannerDoc
from issueplanner.utils import bb_to_planner_ts

def get_options(argv):
    cfg = get_config()

    ap = ArgumentParser(
        description = __doc__.splitlines()[0],
        )
    ap.add_argument(
        "--planner-filename", "-p",
        required=True,
        help="path to planner xml file"
        )
    ap.add_argument(
        "--add-milestones", "-M",
        default=False,
        action="store_true"
        )
    ap.add_argument(
        "--bitbucket-username",
        default=cfg.get("bitbucket","username"),
        )
    ap.add_argument(
        "--bitbucket-password",
        default=cfg.get("bitbucket","password"),
        )
    ap.add_argument(
        "--cache-dir", "-C",
        default=cfg.get("issueplanner", "cache-dir"),
        )
    ap.add_argument(
        "--prefixes", "-P",
        default=[],
        action="append"
        )
    opts = ap.parse_args(argv[1:])
    opts.cache_dir = os.path.expanduser(opts.cache_dir)
    opts._cfg = cfg
    return opts
    

def get_config():
    cfg_fn = os.path.join(os.path.expanduser("~"), ".config", "issueplanner.cfg")
    cfg = ConfigParser()
    with open(cfg_fn,"r") as fp:
        cfg.readfp(fp)
    return cfg


def _get_issues(opts,tracker):
    fqtn = tracker["owner"] + "/" + tracker["slug"]
    pickle_path = os.path.join(opts.cache_dir,fqtn.replace("/","*")+".gz")
    if os.path.exists(pickle_path):
        with gzip.open(pickle_path,"r") as fh:
            issues = pickle.load(fh)
        logger.info("read {n} {fqtn} issues from {fn}".format(
            n=len(issues), fqtn=fqtn, fn=pickle_path))
    else:
        bb = BitbucketClient(username=opts.bitbucket_username,
                             password=opts.bitbucket_password)
        issues = list(bb.get_all_issues(tracker["owner"], tracker["slug"]))
        with gzip.open(pickle_path,"w") as fh:
            pickle.dump(issues,fh)
        logger.info("fetched {n} {fqtn} issues; pickled in {fn}".format(
            n=len(issues), fqtn=fqtn, fn=pickle_path))
    return issues


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    opts = get_options(sys.argv)

    if not os.path.exists(opts.cache_dir):
        os.makedirs(opts.cache_dir)
    
    with open(opts.planner_filename,"r") as fh:
        pd = IssuePlannerDoc(le.fromstring(fh.read()))

    trackers = pd.get_trackers()
    pt_map = {t["prefix"]: t for t in trackers}
    prefixes = opts.prefixes or pt_map.keys()

    all_tasks = pd.tasks()

    for prefix in prefixes:
        logger.info("processing {} issues...".format(prefix))

        pt = pt_map[prefix]
        prj_te = pd.get_project_task(pt)

        # map issue ids => task node elements
        task_issue_re = re.compile(r"(?P<task_title>.*)\s*\[(?P<issue_id>{prefix}-\d+)\]\s*$".format(prefix=prefix))
        planner_tasks_map = {
            m.group("issue_id"): t
            for t,m in ((t, task_issue_re.match(t.get("name"))) for t in all_tasks)
            if m is not None
            }
        planner_tasks_ii = set(planner_tasks_map.keys())

        # map issue ids => issue data
        issues = _get_issues(opts,pt_map[prefix])
        tracker_issues_map = {
            "{}-{}".format(prefix,i["local_id"]): i
            for i in issues
            }
        tracker_issues_ii = set(tracker_issues_map.keys())

        t_not_p = tracker_issues_ii - planner_tasks_ii  # in tracker, not planner
        p_and_t = planner_tasks_ii & tracker_issues_ii  # in both
        p_not_t = planner_tasks_ii - tracker_issues_ii  # in planner, not tracker

        logger.info("{} unique in tracker, {} unique in planner xml, {} common".format(
            len(t_not_p), len(p_not_t), len(p_and_t)))

        if len(p_not_t) > 0:
            logger.warning("{} issue ids in planner are non-existant in the {}/{} issue tracker: {}".format(
                len(p_not_t), pt_map[prefix]["owner"], pt_map[prefix]["slug"], ", ".join(p_not_t)))
            


        # Q: merge add and update cases?

        # create placeholder tasks for issues that don't exist
        # attributes will be updated in the block below
        for ii in t_not_p:
            issue = tracker_issues_map[ii]
            ms = issue['metadata']['milestone']
            parent_te = pd.get_milestone_task(prj_te, ms)
            planner_tasks_map[ii] = i_te = pd.create_task(ii)
            parent_te.append(i_te)
            logger.info("added issue {} to {}".format(ii, parent_te.get("name")))

        # update these tasks
        for ii in p_and_t ^ t_not_p:
            task = planner_tasks_map[ii]
            issue = tracker_issues_map[ii]
            ms = issue['metadata']['milestone']

            task.attrib["name"] = "{it} [{ii}]".format(it=issue["title"], ii=ii)
            task.attrib["start"] = bb_to_planner_ts(issue["created_on"])
            task.attrib["percent-complete"] = "100" if issue["status"] in ["closed"] else "0"

            current_ms_te = task.getparent()
            new_ms_te = pd.get_milestone_task(prj_te, ms)
            if current_ms_te != new_ms_te:
                current_ms_te.remove(task)
                new_ms_te.append(task)
                logger.info("moved issue {} from {} to {}".format(
                    ii, current_ms_te.get("name"), new_ms_te.get("name")))

    print(pd)
