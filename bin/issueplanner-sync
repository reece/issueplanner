#!/usr/bin/env python

from __future__ import absolute_import, division, print_function, unicode_literals

__doc__ = """Update a GNOME planner XML document with data from Bitbucket issues

Desired result: A Gantt chart in which tasks may optionally correspond
to issues in multiple Bitbucket issue trackers and task data may be
refreshed periodically.  Dependency information, which is not
available in Bitbucket, is stored in the planner XML.

> Integration/User-Features
  > Feature1
  > Feature2
> Project A1
  > Milestone A1
    > Task A1.1
    > Task A1.2
  > Milestone A2
    ...


Process notes:
- fetch all issues

- 3 cases: new, updated, deleted
  new: add to milestone or "unplanned" section
    (add milestone if needed... see below)
  updated: update title, kind, dates, status, owner
    (add owner if needed)  
  deleted: log msg only

- adding a milestone: add as dependent on inferred previous

"""

from argparse import ArgumentParser
from ConfigParser import ConfigParser
import cPickle as pickle
import gzip
import io
import logging
import os
import re
import sys
import time

import lxml.etree as le

from issueplanner.bitbucketclient import BitbucketClient
from issueplanner.issueplannerdoc import IssuePlannerDoc
from issueplanner.utils import bb_to_planner_ts, issue_elapsed_work_seconds, issue_sort_key, issue_status_symbols, status_map


def get_options(argv):
    cfg = get_config()

    ap = ArgumentParser(
        description = __doc__.splitlines()[0],
        )
    ap.add_argument(
        "--planner-filename", "-p",
        required=True,
        help="path to planner xml file"
        )
    ap.add_argument(
        "--bitbucket-username",
        default=cfg.get("bitbucket","username"),
        )
    ap.add_argument(
        "--bitbucket-password",
        default=cfg.get("bitbucket","password"),
        )
    ap.add_argument(
        "--cache-dir", "-C",
        default=cfg.get("issueplanner", "cache-dir"),
        )
    ap.add_argument(
        "--update-cache", "-U",
        action="append",
        nargs="?",
        default=None
        )
    ap.add_argument(
        "--output-filename", "-o",
        )
    ap.add_argument(
        "--prefixes", "-P",
        default=[],
        action="append"
        )
    opts = ap.parse_args(argv[1:])
    opts._cfg = cfg
    return opts
    

def get_config():
    cfg_fn = os.path.join(os.path.expanduser("~"), ".config", "issueplanner.cfg")
    cfg = ConfigParser()
    with io.open(cfg_fn,"r") as fp:
        cfg.readfp(fp)
    return cfg


def _get_issues(opts,tracker):
    fqtn = tracker["owner"] + "/" + tracker["slug"]
    pickle_path = os.path.join(opts.cache_dir,fqtn.replace("/","*")+".gz")
    update_cache = (opts.update_cache is not None
                    and (opts.update_cache == [None] or tracker["prefix"] in opts.update_cache))
    if not update_cache and os.path.exists(pickle_path):
        with gzip.open(pickle_path,"r") as fh:
            issues = pickle.load(fh)
        logger.debug("read {n} {fqtn} issues from {fn}".format(
            n=len(issues), fqtn=fqtn, fn=pickle_path))
    else:
        bb = BitbucketClient(username=opts.bitbucket_username,
                             password=opts.bitbucket_password)
        issues = list(bb.get_all_issues(tracker["owner"], tracker["slug"]))
        with gzip.open(pickle_path,"w") as fh:
            pickle.dump(issues,fh)
        logger.debug("fetched {n} {fqtn} issues; pickled in {fn}".format(
            n=len(issues), fqtn=fqtn, fn=pickle_path))

    return issues



def sort_tasks(nodes, it_to_ii_map, ii_issue_map):
    """sort nodes according to issue data"""
    def _task_sort_key(it):
        """return sort key for given issue task"""
        try:
            ii = it_to_ii_map[it]
        except KeyError:
            # task is a sibling of a tracked issue, but is not in
            # tracker
            return None
        iss = ii_issue_map[ii]
        return issue_sort_key(iss)
    nodes.sort(key=_task_sort_key)
    return nodes


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    opts = get_options(sys.argv)
    opts.cache_dir = os.path.expanduser(opts.cache_dir)

    if not os.path.exists(opts.cache_dir):
        os.makedirs(opts.cache_dir)

    with io.open(opts.planner_filename,"r") as fh:
        parser = le.XMLParser(remove_blank_text=True)
        doc = le.parse(fh, parser)
        pd = IssuePlannerDoc(doc.getroot())

    trackers = pd.get_trackers()
    pt_map = {t["prefix"]: t for t in trackers}
    prefixes = opts.prefixes or pt_map.keys()

    all_tasks = pd.tasks()

    for prefix in prefixes:
        pt = pt_map[prefix]
        prj_te = pd.get_project_task(pt)

        # map issue ids => task node elements
        task_issue_re = re.compile(r"\[(?P<issue_id>{prefix}-\d+)\]".format(prefix=prefix))
        ii_te_map = {
            m.group("issue_id"): t
            for t,m in ((t, task_issue_re.search(t.get("name"))) for t in all_tasks)
            if m is not None
            }
        ii_te_keys = set(ii_te_map.keys())

        # map issue ids => issue data
        issues = _get_issues(opts,pt_map[prefix])
        ii_issue_map = {
            "{}-{}".format(prefix,i["local_id"]): i
            for i in issues
            }
        ii_issue_keys = set(ii_issue_map.keys())

        t_not_p = ii_issue_keys - ii_te_keys  # in tracker, not planner
        p_and_t = ii_te_keys & ii_issue_keys  # in both
        p_not_t = ii_te_keys - ii_issue_keys  # in planner, not tracker

        logger.info("{}: {} unique in tracker, {} unique in planner xml, {} common".format(
            prefix, len(t_not_p), len(p_not_t), len(p_and_t)))

        if len(p_not_t) > 0:
            logger.warning("{} issue ids in planner are non-existant in the {}/{} issue tracker: {}".format(
                len(p_not_t), pt_map[prefix]["owner"], pt_map[prefix]["slug"], ", ".join(p_not_t)))
            

        # create placeholder tasks for issues that don't exist
        # attributes will be updated in the block below
        for ii in t_not_p:
            issue = ii_issue_map[ii]
            i_te = pd.create_task(ii)
            prj_te.append(i_te)
            ii_te_map[ii] = i_te
            logger.info("added issue {}".format(ii))

        # update these tasks (t_not_p added above)
        task_issue_ii = p_and_t ^ t_not_p
        for ii in task_issue_ii:
            task = ii_te_map[ii]
            issue = ii_issue_map[ii]

            task.attrib["name"] = "[{ii}] {iss} {it}".format(
                it=issue["title"], ii=ii, iss=issue_status_symbols(issue))

            # set dates and completion
            if not task.attrib["start"]:
                task.attrib["start"] = bb_to_planner_ts(issue["utc_created_on"])
                pd.set_task_constraint(task, "start-no-earlier-than", task.attrib["start"])
            if not task.attrib["work-start"]:
                task.attrib["work-start"] = task.attrib["start"]
            if not task.attrib["end"]:
                task.attrib["end"] = bb_to_planner_ts(issue["utc_last_updated"])
            
            task.attrib["work"] = unicode(issue_elapsed_work_seconds(issue))
            task.attrib["percent-complete"] = status_map[issue["status"]]["percent-complete"]

            # move task to correct milestone if neccessary
            ms = issue['metadata']['milestone']
            current_te = task.getparent()
            new_ms_te = pd.get_milestone_task(prj_te, ms)
            new_te = pd.get_task_path(new_ms_te, [issue['metadata']['kind']])
            if current_te != new_te:
                current_te.remove(task)
                new_te.append(task)
                logger.info("moved issue {} from {} to {}".format(
                    ii, current_te.get("name"), new_te.get("name")))

            # TODO: assign/unassign task
            

        # sort planner tasks by tracker issue info via issue ids (ii)
        # this requires a map from te (task element) to ii
        # and a second map from ii to issue info
        te_ii_map = {ii_te_map[ii]:ii for ii in task_issue_ii}
        parents_of_issue_tasks = set(te.getparent() for te in ii_te_map.values())
        for itp_te in parents_of_issue_tasks:
            itp_te[:] = sort_tasks(itp_te[:], te_ii_map, ii_issue_map)


    pd.reset_project_start()

    out_fn = opts.output_filename or opts.planner_filename
    if os.path.exists(out_fn):
        os.rename(out_fn, out_fn + "-" + str(int(time.time())))
    with io.open(out_fn,"w") as fh:
        fh.write(unicode(pd))
    logger.info("wrote {out_fn}".format(out_fn=out_fn))

